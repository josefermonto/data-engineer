# 9. Data Quality y Governance

## üìã Tabla de Contenidos
- [Fundamentos de Data Quality](#fundamentos-de-data-quality)
- [Data Validation y Testing](#data-validation-y-testing)
- [Data Governance](#data-governance)
- [Data Lineage](#data-lineage)
- [Metadata Management](#metadata-management)
- [Data Catalogs](#data-catalogs)
- [Compliance y Regulaciones](#compliance-y-regulaciones)
- [Herramientas y Frameworks](#herramientas-y-frameworks)

---

## Fundamentos de Data Quality

### Dimensiones de Data Quality

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          6 DIMENSIONES DE DATA QUALITY              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  1. ACCURACY (Precisi√≥n)                            ‚îÇ
‚îÇ     ¬øLos datos son correctos?                       ‚îÇ
‚îÇ     Ejemplo: email='user@example..com' ‚ùå           ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  2. COMPLETENESS (Completitud)                      ‚îÇ
‚îÇ     ¬øHay valores faltantes?                         ‚îÇ
‚îÇ     Ejemplo: customer_name=NULL ‚ùå                  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  3. CONSISTENCY (Consistencia)                      ‚îÇ
‚îÇ     ¬øDatos contradictorios entre sistemas?          ‚îÇ
‚îÇ     Ejemplo: age=25 pero birthdate=1980 ‚ùå          ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  4. TIMELINESS (Puntualidad)                        ‚îÇ
‚îÇ     ¬øDatos actualizados?                            ‚îÇ
‚îÇ     Ejemplo: dashboard muestra datos de ayer ‚ùå     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  5. VALIDITY (Validez)                              ‚îÇ
‚îÇ     ¬øCumplen reglas de negocio?                     ‚îÇ
‚îÇ     Ejemplo: amount=-100 para una venta ‚ùå          ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  6. UNIQUENESS (Unicidad)                           ‚îÇ
‚îÇ     ¬øHay duplicados?                                ‚îÇ
‚îÇ     Ejemplo: mismo customer_id 3 veces ‚ùå           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Quality Metrics

```python
import pandas as pd
import numpy as np

def calculate_dq_metrics(df: pd.DataFrame, unique_cols: list = None) -> dict:
    """
    Calcular m√©tricas de data quality para un DataFrame
    """
    metrics = {}

    # 1. COMPLETENESS
    total_cells = df.size
    missing_cells = df.isnull().sum().sum()
    metrics['completeness_pct'] = 100 * (1 - missing_cells / total_cells)

    # Por columna
    metrics['completeness_by_col'] = {}
    for col in df.columns:
        non_null = df[col].notna().sum()
        metrics['completeness_by_col'][col] = 100 * non_null / len(df)

    # 2. UNIQUENESS
    if unique_cols:
        duplicates = df.duplicated(subset=unique_cols).sum()
        metrics['uniqueness_pct'] = 100 * (1 - duplicates / len(df))
        metrics['duplicate_count'] = duplicates

    # 3. VALIDITY (ejemplo: rangos num√©ricos)
    metrics['validity_checks'] = {}
    for col in df.select_dtypes(include=[np.number]).columns:
        # Valores negativos en columnas que deber√≠an ser positivas
        if col in ['amount', 'price', 'quantity']:
            invalid_count = (df[col] < 0).sum()
            metrics['validity_checks'][f'{col}_negative'] = {
                'invalid_count': invalid_count,
                'invalid_pct': 100 * invalid_count / len(df)
            }

    # 4. TIMELINESS
    if 'updated_at' in df.columns:
        from datetime import datetime, timedelta
        now = datetime.now()
        stale_threshold = now - timedelta(days=7)

        df['updated_at'] = pd.to_datetime(df['updated_at'])
        stale_count = (df['updated_at'] < stale_threshold).sum()
        metrics['timeliness'] = {
            'stale_count': stale_count,
            'stale_pct': 100 * stale_count / len(df)
        }

    return metrics

# Uso
df = pd.read_csv('customers.csv')
metrics = calculate_dq_metrics(df, unique_cols=['customer_id'])

print(f"Completeness: {metrics['completeness_pct']:.2f}%")
print(f"Uniqueness: {metrics['uniqueness_pct']:.2f}%")
print(f"Duplicates: {metrics['duplicate_count']}")
```

---

## Data Validation y Testing

### 1. Great Expectations

**Caracter√≠sticas:**
- Framework de validaci√≥n de datos en Python
- Expectativas reutilizables
- Generaci√≥n autom√°tica de documentaci√≥n
- Integraci√≥n con Airflow, dbt, Spark

**Instalaci√≥n y Setup:**

```bash
pip install great-expectations
great_expectations init  # Crear proyecto
```

**Ejemplo Completo:**

```python
import great_expectations as ge
from great_expectations.dataset import PandasDataset
import pandas as pd

# ===== 1. CREAR EXPECTATIONS =====
df = pd.read_csv('sales.csv')
ge_df = ge.from_pandas(df)

# Expectativas b√°sicas
expectations = [
    # Columnas requeridas
    ge_df.expect_table_columns_to_match_ordered_list([
        'order_id', 'customer_id', 'amount', 'order_date'
    ]),

    # No nulls en columnas cr√≠ticas
    ge_df.expect_column_values_to_not_be_null('order_id'),
    ge_df.expect_column_values_to_not_be_null('customer_id'),

    # Unicidad
    ge_df.expect_column_values_to_be_unique('order_id'),

    # Rangos v√°lidos
    ge_df.expect_column_values_to_be_between('amount', min_value=0, max_value=100000),

    # Valores en set
    ge_df.expect_column_values_to_be_in_set('status', ['pending', 'completed', 'cancelled']),

    # Formato de email
    ge_df.expect_column_values_to_match_regex('email', r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'),

    # Fecha en rango
    ge_df.expect_column_values_to_be_between(
        'order_date',
        min_value='2020-01-01',
        max_value='2025-12-31'
    ),

    # Suma de columna
    ge_df.expect_column_sum_to_be_between('amount', min_value=1000000),

    # Conteo de filas
    ge_df.expect_table_row_count_to_be_between(min_value=100, max_value=1000000)
]

# ===== 2. GUARDAR EXPECTATIONS SUITE =====
from great_expectations.core.batch import RuntimeBatchRequest

context = ge.get_context()

# Crear expectation suite
suite = context.create_expectation_suite(
    expectation_suite_name="sales_validation",
    overwrite_existing=True
)

# Agregar expectations
for expectation in expectations:
    suite.add_expectation(expectation)

context.save_expectation_suite(suite)

# ===== 3. VALIDAR DATOS =====
def validate_sales_data(df: pd.DataFrame) -> dict:
    """
    Validar DataFrame contra expectation suite
    """
    context = ge.get_context()

    # Crear batch
    batch = context.get_validator(
        batch_request=RuntimeBatchRequest(
            datasource_name="pandas_datasource",
            data_connector_name="default_runtime_data_connector_name",
            data_asset_name="sales",
            runtime_parameters={"batch_data": df},
            batch_identifiers={"default_identifier_name": "sales_batch"}
        ),
        expectation_suite_name="sales_validation"
    )

    # Validar
    results = batch.validate()

    return {
        'success': results.success,
        'total_expectations': len(results.results),
        'successful_expectations': sum([r.success for r in results.results]),
        'failed_expectations': [
            {
                'expectation': r.expectation_config.expectation_type,
                'column': r.expectation_config.kwargs.get('column'),
                'details': r.result
            }
            for r in results.results if not r.success
        ]
    }

# Validar nuevo batch
new_df = pd.read_csv('sales_today.csv')
validation_results = validate_sales_data(new_df)

if not validation_results['success']:
    print("‚ùå Validaci√≥n fall√≥:")
    for failure in validation_results['failed_expectations']:
        print(f"  - {failure['expectation']} en columna {failure['column']}")
        print(f"    {failure['details']}")
else:
    print("‚úÖ Validaci√≥n exitosa")

# ===== 4. INTEGRACI√ìN CON AIRFLOW =====
from airflow import DAG
from airflow.operators.python import PythonOperator
from great_expectations_provider.operators.great_expectations import GreatExpectationsOperator

dag = DAG('sales_etl_with_validation', ...)

validate_task = GreatExpectationsOperator(
    task_id='validate_sales',
    expectation_suite_name='sales_validation',
    batch_kwargs={
        'path': 's3://data/sales/{{ ds }}.csv',
        'datasource': 's3_datasource'
    },
    fail_task_on_validation_failure=True,  # Fallar task si validaci√≥n falla
    dag=dag
)

load_task = PythonOperator(...)

validate_task >> load_task  # Solo carga si validaci√≥n pasa
```

### 2. Custom Validation Functions

```python
from typing import List, Dict, Any
import pandas as pd

class DataValidator:
    """
    Validador custom de datos con reglas de negocio
    """

    def __init__(self):
        self.errors = []
        self.warnings = []

    def validate_sales_data(self, df: pd.DataFrame) -> bool:
        """
        Validaciones custom para datos de ventas
        """
        self.errors = []
        self.warnings = []

        # 1. Schema validation
        required_columns = ['order_id', 'customer_id', 'amount', 'order_date']
        missing_cols = set(required_columns) - set(df.columns)
        if missing_cols:
            self.errors.append(f"Columnas faltantes: {missing_cols}")
            return False

        # 2. Business rules
        # Regla: amount debe ser positivo
        negative_amounts = df[df['amount'] <= 0]
        if len(negative_amounts) > 0:
            self.errors.append(
                f"{len(negative_amounts)} √≥rdenes con amount <= 0: {negative_amounts['order_id'].tolist()[:5]}"
            )

        # Regla: order_date no puede ser futuro
        df['order_date'] = pd.to_datetime(df['order_date'])
        future_dates = df[df['order_date'] > pd.Timestamp.now()]
        if len(future_dates) > 0:
            self.errors.append(
                f"{len(future_dates)} √≥rdenes con fecha futura: {future_dates['order_id'].tolist()[:5]}"
            )

        # 3. Referential integrity
        # Regla: customer_id debe existir en tabla customers
        # (asumiendo que tenemos acceso a tabla customers)
        # invalid_customers = df[~df['customer_id'].isin(valid_customer_ids)]

        # 4. Data consistency
        # Regla: customer_email debe matchear formato
        if 'email' in df.columns:
            invalid_emails = df[~df['email'].str.match(r'^[^@]+@[^@]+\.[^@]+$')]
            if len(invalid_emails) > 0:
                self.warnings.append(
                    f"{len(invalid_emails)} emails con formato inv√°lido"
                )

        # 5. Statistical validation
        # Regla: amount no debe ser outlier extremo (> 5 std)
        mean_amount = df['amount'].mean()
        std_amount = df['amount'].std()
        outliers = df[df['amount'] > mean_amount + 5 * std_amount]
        if len(outliers) > 0:
            self.warnings.append(
                f"{len(outliers)} √≥rdenes con amount outlier: max={outliers['amount'].max()}"
            )

        # 6. Duplicate check
        duplicates = df[df.duplicated(subset=['order_id'])]
        if len(duplicates) > 0:
            self.errors.append(
                f"{len(duplicates)} order_id duplicados"
            )

        return len(self.errors) == 0

    def get_report(self) -> Dict[str, Any]:
        """
        Generar reporte de validaci√≥n
        """
        return {
            'is_valid': len(self.errors) == 0,
            'error_count': len(self.errors),
            'warning_count': len(self.warnings),
            'errors': self.errors,
            'warnings': self.warnings
        }

# Uso
validator = DataValidator()
df = pd.read_csv('sales.csv')

if validator.validate_sales_data(df):
    print("‚úÖ Datos v√°lidos")
    # Proceder con ETL
else:
    print("‚ùå Datos inv√°lidos:")
    report = validator.get_report()
    for error in report['errors']:
        print(f"  - {error}")
```

### 3. SQL Data Quality Checks

```sql
-- ===== VALIDACIONES EN SQL =====

-- 1. COMPLETENESS: Detectar nulls
SELECT
    'customer_id' as column_name,
    COUNT(*) as total_rows,
    COUNT(customer_id) as non_null_rows,
    COUNT(*) - COUNT(customer_id) as null_count,
    100.0 * COUNT(customer_id) / COUNT(*) as completeness_pct
FROM customers
UNION ALL
SELECT
    'email',
    COUNT(*),
    COUNT(email),
    COUNT(*) - COUNT(email),
    100.0 * COUNT(email) / COUNT(*)
FROM customers;

-- 2. UNIQUENESS: Detectar duplicados
SELECT
    customer_id,
    COUNT(*) as duplicate_count
FROM customers
GROUP BY customer_id
HAVING COUNT(*) > 1;

-- 3. VALIDITY: Detectar valores fuera de rango
SELECT
    COUNT(*) as invalid_amount_count
FROM orders
WHERE amount < 0 OR amount > 1000000;

-- 4. REFERENTIAL INTEGRITY
-- √ìrdenes con customer_id que no existe
SELECT o.order_id, o.customer_id
FROM orders o
LEFT JOIN customers c ON o.customer_id = c.customer_id
WHERE c.customer_id IS NULL;

-- 5. CONSISTENCY: Cross-table checks
-- Verificar que total en orders_summary = SUM(orders)
SELECT
    os.summary_date,
    os.total_amount as summary_total,
    COALESCE(SUM(o.amount), 0) as actual_total,
    os.total_amount - COALESCE(SUM(o.amount), 0) as difference
FROM orders_summary os
LEFT JOIN orders o ON DATE(o.order_date) = os.summary_date
GROUP BY os.summary_date, os.total_amount
HAVING ABS(os.total_amount - COALESCE(SUM(o.amount), 0)) > 0.01;

-- 6. TIMELINESS: Detectar datos stale
SELECT
    COUNT(*) as stale_records
FROM customers
WHERE updated_at < CURRENT_DATE - INTERVAL '30 days';

-- ===== CREAR TABLA DE M√âTRICAS DQ =====
CREATE TABLE data_quality_metrics (
    metric_date DATE,
    table_name VARCHAR(100),
    metric_name VARCHAR(100),
    metric_value DECIMAL(10,2),
    status VARCHAR(20),  -- 'PASS' or 'FAIL'
    details TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Insertar m√©tricas diarias
INSERT INTO data_quality_metrics
SELECT
    CURRENT_DATE as metric_date,
    'customers' as table_name,
    'completeness_email' as metric_name,
    100.0 * COUNT(email) / COUNT(*) as metric_value,
    CASE
        WHEN 100.0 * COUNT(email) / COUNT(*) >= 95 THEN 'PASS'
        ELSE 'FAIL'
    END as status,
    CONCAT(COUNT(*) - COUNT(email), ' null values') as details,
    CURRENT_TIMESTAMP
FROM customers;

-- Dashboard de DQ metrics
SELECT
    metric_date,
    table_name,
    metric_name,
    metric_value,
    status
FROM data_quality_metrics
WHERE metric_date >= CURRENT_DATE - INTERVAL '7 days'
ORDER BY metric_date DESC, table_name, metric_name;
```

---

## Data Governance

### Principios de Data Governance

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           DATA GOVERNANCE FRAMEWORK                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                    ‚îÇ
‚îÇ  1. DATA OWNERSHIP                                 ‚îÇ
‚îÇ     ‚Ä¢ Data Stewards por domain                     ‚îÇ
‚îÇ     ‚Ä¢ Responsabilidad clara                        ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  2. DATA POLICIES                                  ‚îÇ
‚îÇ     ‚Ä¢ Retention policies (cu√°nto guardar)          ‚îÇ
‚îÇ     ‚Ä¢ Access policies (qui√©n puede acceder)        ‚îÇ
‚îÇ     ‚Ä¢ Privacy policies (PII, GDPR)                 ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  3. DATA STANDARDS                                 ‚îÇ
‚îÇ     ‚Ä¢ Naming conventions                           ‚îÇ
‚îÇ     ‚Ä¢ Data types consistency                       ‚îÇ
‚îÇ     ‚Ä¢ Documentation requirements                   ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  4. DATA QUALITY RULES                             ‚îÇ
‚îÇ     ‚Ä¢ Validation requirements                      ‚îÇ
‚îÇ     ‚Ä¢ Quality thresholds                           ‚îÇ
‚îÇ     ‚Ä¢ Monitoring and alerting                      ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  5. DATA LIFECYCLE                                 ‚îÇ
‚îÇ     ‚Ä¢ Creation ‚Üí Usage ‚Üí Archival ‚Üí Deletion       ‚îÇ
‚îÇ     ‚Ä¢ Compliance en cada fase                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Classification

```python
from enum import Enum
from dataclasses import dataclass
from typing import List

class DataSensitivity(Enum):
    PUBLIC = "PUBLIC"           # Datos p√∫blicos
    INTERNAL = "INTERNAL"       # Datos internos
    CONFIDENTIAL = "CONFIDENTIAL"  # Datos confidenciales
    RESTRICTED = "RESTRICTED"   # Datos altamente sensibles (PII, PHI)

class DataRetention(Enum):
    SHORT = 90    # 90 d√≠as
    MEDIUM = 365  # 1 a√±o
    LONG = 2555   # 7 a√±os
    PERMANENT = -1  # Permanente

@dataclass
class DataAsset:
    """
    Metadata de un asset de datos
    """
    name: str
    database: str
    schema: str
    table: str
    owner: str
    sensitivity: DataSensitivity
    retention_days: DataRetention
    pii_columns: List[str]
    description: str
    tags: List[str]

    def is_pii_data(self) -> bool:
        return len(self.pii_columns) > 0

    def requires_encryption(self) -> bool:
        return self.sensitivity in [DataSensitivity.CONFIDENTIAL, DataSensitivity.RESTRICTED]

    def requires_audit_log(self) -> bool:
        return self.sensitivity == DataSensitivity.RESTRICTED

# Ejemplo: Definir assets
customers_table = DataAsset(
    name="customers",
    database="analytics",
    schema="public",
    table="customers",
    owner="data-team@company.com",
    sensitivity=DataSensitivity.RESTRICTED,
    retention_days=DataRetention.LONG,
    pii_columns=["email", "phone", "ssn", "address"],
    description="Customer master data including PII",
    tags=["pii", "gdpr", "critical"]
)

sales_table = DataAsset(
    name="sales",
    database="analytics",
    schema="public",
    table="sales",
    owner="data-team@company.com",
    sensitivity=DataSensitivity.INTERNAL,
    retention_days=DataRetention.LONG,
    pii_columns=[],
    description="Sales transactions (no PII)",
    tags=["financial", "core"]
)

# ===== APLICAR POL√çTICAS =====
def apply_data_policies(asset: DataAsset):
    """
    Aplicar pol√≠ticas de governance a un asset
    """
    policies = []

    # Encryption
    if asset.requires_encryption():
        policies.append({
            'policy': 'ENCRYPTION',
            'action': f"Enable encryption for {asset.name}"
        })

    # Access control
    if asset.is_pii_data():
        policies.append({
            'policy': 'ACCESS_CONTROL',
            'action': f"Restrict access to {asset.name}, require approval"
        })

    # Audit logging
    if asset.requires_audit_log():
        policies.append({
            'policy': 'AUDIT_LOG',
            'action': f"Enable query audit log for {asset.name}"
        })

    # Retention
    if asset.retention_days != DataRetention.PERMANENT:
        policies.append({
            'policy': 'RETENTION',
            'action': f"Delete records older than {asset.retention_days.value} days"
        })

    # Masking PII in non-prod
    if asset.is_pii_data():
        policies.append({
            'policy': 'PII_MASKING',
            'action': f"Mask columns {asset.pii_columns} in dev/test environments"
        })

    return policies

# Aplicar pol√≠ticas
policies = apply_data_policies(customers_table)
for policy in policies:
    print(f"[{policy['policy']}] {policy['action']}")
```

### Access Control (RBAC)

```sql
-- ===== SNOWFLAKE: Role-Based Access Control =====

-- 1. Crear roles
CREATE ROLE data_analyst;
CREATE ROLE data_engineer;
CREATE ROLE data_scientist;
CREATE ROLE pii_viewer;  -- Acceso especial a PII

-- 2. Asignar permisos a roles
-- Data Analyst: Solo lectura en analytics
GRANT USAGE ON DATABASE analytics TO ROLE data_analyst;
GRANT USAGE ON SCHEMA analytics.public TO ROLE data_analyst;
GRANT SELECT ON ALL TABLES IN SCHEMA analytics.public TO ROLE data_analyst;

-- Data Engineer: Lectura/escritura en staging y analytics
GRANT USAGE ON DATABASE staging TO ROLE data_engineer;
GRANT USAGE ON SCHEMA staging.raw TO ROLE data_engineer;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA staging.raw TO ROLE data_engineer;

-- PII Viewer: Acceso a columnas PII (solo usuarios aprobados)
GRANT SELECT ON analytics.public.customers TO ROLE pii_viewer;

-- 3. Asignar roles a usuarios
GRANT ROLE data_analyst TO USER john@company.com;
GRANT ROLE data_engineer TO USER jane@company.com;
GRANT ROLE pii_viewer TO USER compliance_officer@company.com;

-- 4. Row-Level Security
-- Solo ver clientes de tu regi√≥n
CREATE ROW ACCESS POLICY customer_region_policy AS (region STRING)
    RETURNS BOOLEAN ->
        CURRENT_ROLE() = 'ADMIN' OR
        region = CURRENT_USER_REGION();

ALTER TABLE customers ADD ROW ACCESS POLICY customer_region_policy ON (region);

-- 5. Column-Level Security (Masking)
-- Ocultar email para usuarios sin permiso PII
CREATE MASKING POLICY email_mask AS (val STRING)
    RETURNS STRING ->
        CASE
            WHEN CURRENT_ROLE() IN ('pii_viewer', 'ADMIN') THEN val
            ELSE '***@***.com'
        END;

ALTER TABLE customers MODIFY COLUMN email SET MASKING POLICY email_mask;

-- Ahora queries muestran email masked para usuarios sin permiso:
-- data_analyst: SELECT email FROM customers ‚Üí '***@***.com'
-- pii_viewer:   SELECT email FROM customers ‚Üí 'user@example.com'
```

---

## Data Lineage

### ¬øQu√© es Data Lineage?

**Data Lineage** es el rastreo de datos desde su origen hasta su destino, mostrando todas las transformaciones en el camino.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DATA LINEAGE EXAMPLE                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  Source                Transform         Destination‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Shopify  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Glue   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  S3     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   API    ‚îÇ         ‚îÇ  ETL    ‚îÇ      ‚îÇ  Raw    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                             ‚îÇ                ‚îÇ      ‚îÇ
‚îÇ                             ‚îÇ                ‚îÇ      ‚îÇ
‚îÇ                             ‚ñº                ‚ñº      ‚îÇ
‚îÇ                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                       ‚îÇ  Spark  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  S3     ‚îÇ  ‚îÇ
‚îÇ                       ‚îÇTransform‚îÇ      ‚îÇProcessed‚îÇ  ‚îÇ
‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                             ‚îÇ                       ‚îÇ
‚îÇ                             ‚ñº                       ‚îÇ
‚îÇ                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ                       ‚îÇRedshift ‚îÇ                   ‚îÇ
‚îÇ                       ‚îÇAnalytics‚îÇ                   ‚îÇ
‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                             ‚îÇ                       ‚îÇ
‚îÇ                             ‚ñº                       ‚îÇ
‚îÇ                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ                       ‚îÇTableau  ‚îÇ                   ‚îÇ
‚îÇ                       ‚îÇDashboard‚îÇ                   ‚îÇ
‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Capturar Lineage Autom√°ticamente

```python
import json
from datetime import datetime
from typing import List, Dict

class LineageTracker:
    """
    Rastrear lineage de transformaciones
    """

    def __init__(self):
        self.lineage_events = []

    def track_transformation(
        self,
        source_tables: List[str],
        target_table: str,
        transformation_type: str,
        transformation_logic: str,
        metadata: Dict = None
    ):
        """
        Registrar un evento de transformaci√≥n
        """
        event = {
            'timestamp': datetime.utcnow().isoformat(),
            'source_tables': source_tables,
            'target_table': target_table,
            'transformation_type': transformation_type,
            'transformation_logic': transformation_logic,
            'metadata': metadata or {}
        }

        self.lineage_events.append(event)

        # Persistir a storage (S3, DB, etc.)
        self._persist_lineage(event)

    def _persist_lineage(self, event: dict):
        """
        Guardar evento de lineage
        """
        # Ejemplo: Escribir a S3
        import boto3
        s3 = boto3.client('s3')

        key = f"lineage/{event['target_table']}/{event['timestamp']}.json"
        s3.put_object(
            Bucket='data-lineage',
            Key=key,
            Body=json.dumps(event)
        )

# ===== USO EN ETL =====
lineage = LineageTracker()

# Ejemplo: ETL con PySpark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ETL with Lineage").getOrCreate()

# Leer fuentes
customers = spark.read.table("raw.customers")
orders = spark.read.table("raw.orders")

# Track lineage
lineage.track_transformation(
    source_tables=["raw.customers", "raw.orders"],
    target_table="analytics.customer_summary",
    transformation_type="aggregation_join",
    transformation_logic="""
        SELECT
            c.customer_id,
            c.customer_name,
            COUNT(o.order_id) as order_count,
            SUM(o.amount) as total_spent
        FROM customers c
        LEFT JOIN orders o ON c.customer_id = o.customer_id
        GROUP BY c.customer_id, c.customer_name
    """,
    metadata={
        'spark_version': spark.version,
        'executor_count': spark.sparkContext.defaultParallelism,
        'row_count': customers.count() + orders.count()
    }
)

# Transformaci√≥n
result = customers.join(orders, "customer_id") \
    .groupBy("customer_id", "customer_name") \
    .agg(...)

# Escribir resultado
result.write.saveAsTable("analytics.customer_summary")
```

### Herramientas de Lineage

**1. Apache Atlas**
- Open-source metadata and governance
- Integraci√≥n con Hadoop ecosystem

**2. Marquez (OpenLineage)**
- Open-source lineage collection
- Integraci√≥n con Airflow, dbt, Spark

**3. Alation**
- Data catalog comercial con lineage
- ML-powered automation

**4. Collibra**
- Enterprise data governance platform
- End-to-end lineage

---

## Metadata Management

### Tipos de Metadata

| Tipo | Descripci√≥n | Ejemplo |
|------|-------------|---------|
| **Technical** | Estructura de datos | Schema, types, constraints |
| **Business** | Significado de negocio | Definiciones, ownership |
| **Operational** | Datos de operaci√≥n | Size, row count, last updated |
| **Lineage** | Origen y transformaciones | Source ‚Üí Transform ‚Üí Target |

### Metadata Store

```python
from dataclasses import dataclass, asdict
from typing import List, Optional
from datetime import datetime
import json

@dataclass
class TableMetadata:
    """
    Metadata completo de una tabla
    """
    # Technical metadata
    database: str
    schema: str
    table_name: str
    columns: List[dict]  # [{'name': 'col1', 'type': 'string', ...}]
    primary_key: Optional[List[str]]
    foreign_keys: List[dict]

    # Business metadata
    description: str
    owner: str
    steward: str
    domain: str  # finance, marketing, sales, etc.

    # Operational metadata
    row_count: int
    size_bytes: int
    last_updated: datetime
    update_frequency: str  # daily, hourly, etc.

    # Governance metadata
    sensitivity: str  # public, internal, confidential, restricted
    retention_days: int
    pii_columns: List[str]
    tags: List[str]

    # Lineage metadata
    source_tables: List[str]
    downstream_tables: List[str]

    def to_json(self) -> str:
        data = asdict(self)
        data['last_updated'] = self.last_updated.isoformat()
        return json.dumps(data, indent=2)

# ===== EJEMPLO: Metadata de tabla customers =====
customers_metadata = TableMetadata(
    database="analytics",
    schema="public",
    table_name="customers",
    columns=[
        {'name': 'customer_id', 'type': 'VARCHAR(50)', 'nullable': False},
        {'name': 'email', 'type': 'VARCHAR(255)', 'nullable': False},
        {'name': 'name', 'type': 'VARCHAR(100)', 'nullable': True},
        {'name': 'created_at', 'type': 'TIMESTAMP', 'nullable': False},
    ],
    primary_key=['customer_id'],
    foreign_keys=[],
    description="Customer master data including contact information",
    owner="data-team@company.com",
    steward="john.doe@company.com",
    domain="sales",
    row_count=1_500_000,
    size_bytes=350_000_000,
    last_updated=datetime.utcnow(),
    update_frequency="daily",
    sensitivity="restricted",
    retention_days=2555,  # 7 years
    pii_columns=['email', 'name', 'phone'],
    tags=['core', 'pii', 'gdpr'],
    source_tables=['raw.shopify_customers', 'raw.salesforce_contacts'],
    downstream_tables=['analytics.customer_segments', 'ml.churn_features']
)

print(customers_metadata.to_json())

# ===== METADATA CATALOG (centralizado) =====
class MetadataCatalog:
    """
    Cat√°logo centralizado de metadata
    """

    def __init__(self, storage_backend='dynamodb'):
        self.backend = storage_backend

    def register_table(self, metadata: TableMetadata):
        """
        Registrar metadata de tabla en cat√°logo
        """
        if self.backend == 'dynamodb':
            import boto3
            dynamodb = boto3.resource('dynamodb')
            table = dynamodb.Table('metadata_catalog')

            table.put_item(Item={
                'table_id': f"{metadata.database}.{metadata.schema}.{metadata.table_name}",
                'metadata': metadata.to_json(),
                'last_updated': metadata.last_updated.isoformat()
            })

    def get_table_metadata(self, database: str, schema: str, table_name: str) -> Optional[TableMetadata]:
        """
        Obtener metadata de tabla
        """
        # Implementaci√≥n depende del backend
        pass

    def search_tables(self, tags: List[str] = None, domain: str = None) -> List[TableMetadata]:
        """
        Buscar tablas por tags o domain
        """
        pass

# Uso
catalog = MetadataCatalog()
catalog.register_table(customers_metadata)
```

---

## Data Catalogs

### AWS Glue Data Catalog

```python
import boto3

glue = boto3.client('glue')

# ===== BUSCAR TABLAS =====
def search_tables(database: str, keyword: str = None):
    """
    Buscar tablas en Glue Catalog
    """
    paginator = glue.get_paginator('get_tables')

    for page in paginator.paginate(DatabaseName=database):
        for table in page['TableList']:
            if keyword is None or keyword.lower() in table['Name'].lower():
                print(f"Table: {table['Name']}")
                print(f"  Location: {table['StorageDescriptor']['Location']}")
                print(f"  Columns: {len(table['StorageDescriptor']['Columns'])}")
                print()

# ===== AGREGAR METADATA CUSTOM =====
def add_table_metadata(database: str, table: str, metadata: dict):
    """
    Agregar metadata custom a tabla en Glue Catalog
    """
    response = glue.get_table(DatabaseName=database, Name=table)
    table_input = response['Table']

    # Agregar parameters (metadata custom)
    if 'Parameters' not in table_input:
        table_input['Parameters'] = {}

    table_input['Parameters'].update({
        'owner': metadata.get('owner', ''),
        'pii_columns': ','.join(metadata.get('pii_columns', [])),
        'sensitivity': metadata.get('sensitivity', ''),
        'retention_days': str(metadata.get('retention_days', 0))
    })

    # Actualizar tabla
    glue.update_table(
        DatabaseName=database,
        TableInput=table_input
    )

# Uso
add_table_metadata('analytics', 'customers', {
    'owner': 'data-team@company.com',
    'pii_columns': ['email', 'phone'],
    'sensitivity': 'restricted',
    'retention_days': 2555
})

# ===== BUSCAR POR METADATA =====
def find_pii_tables(database: str):
    """
    Encontrar todas las tablas que contienen PII
    """
    response = glue.get_tables(DatabaseName=database)

    pii_tables = []
    for table in response['TableList']:
        params = table.get('Parameters', {})
        if 'pii_columns' in params and params['pii_columns']:
            pii_tables.append({
                'table': table['Name'],
                'pii_columns': params['pii_columns'].split(',')
            })

    return pii_tables

pii_tables = find_pii_tables('analytics')
for table in pii_tables:
    print(f"{table['table']}: {table['pii_columns']}")
```

---

## Compliance y Regulaciones

### GDPR (General Data Protection Regulation)

**Principios clave:**

1. **Right to Access** - Usuario puede solicitar sus datos
2. **Right to Erasure** (Right to be Forgotten) - Eliminar datos de usuario
3. **Data Portability** - Exportar datos en formato portable
4. **Data Minimization** - Solo recolectar datos necesarios
5. **Privacy by Design** - Privacy integrado desde dise√±o

**Implementaci√≥n:**

```python
# ===== RIGHT TO ACCESS: Exportar datos de usuario =====
def export_user_data(user_id: str) -> dict:
    """
    Exportar todos los datos de un usuario (GDPR Art. 15)
    """
    from snowflake.connector import connect

    conn = connect(...)
    cursor = conn.cursor()

    user_data = {}

    # Buscar en todas las tablas con PII
    pii_tables = [
        'customers',
        'orders',
        'support_tickets',
        'marketing_preferences'
    ]

    for table in pii_tables:
        cursor.execute(f"""
            SELECT *
            FROM {table}
            WHERE user_id = %s
        """, (user_id,))

        columns = [desc[0] for desc in cursor.description]
        rows = cursor.fetchall()

        user_data[table] = [
            dict(zip(columns, row))
            for row in rows
        ]

    return user_data

# ===== RIGHT TO ERASURE: Eliminar datos de usuario =====
def delete_user_data(user_id: str, reason: str):
    """
    Eliminar todos los datos de un usuario (GDPR Art. 17)
    """
    from snowflake.connector import connect
    import logging

    conn = connect(...)
    cursor = conn.cursor()

    # Log de auditor√≠a
    logging.info(f"Deleting data for user {user_id}, reason: {reason}")

    try:
        cursor.execute("BEGIN")

        # Eliminar o anonimizar en cada tabla
        tables_to_delete = [
            'customers',
            'orders',
            'support_tickets'
        ]

        for table in tables_to_delete:
            cursor.execute(f"""
                DELETE FROM {table}
                WHERE user_id = %s
            """, (user_id,))

        # Anonimizar (en vez de eliminar) en tablas de analytics
        cursor.execute("""
            UPDATE analytics.user_behavior
            SET user_id = 'DELETED_USER',
                email = 'deleted@deleted.com'
            WHERE user_id = %s
        """, (user_id,))

        # Guardar registro en tabla de auditor√≠a
        cursor.execute("""
            INSERT INTO gdpr_deletion_log (user_id, deleted_at, reason)
            VALUES (%s, CURRENT_TIMESTAMP, %s)
        """, (user_id, reason))

        cursor.execute("COMMIT")
        logging.info(f"Successfully deleted data for user {user_id}")

    except Exception as e:
        cursor.execute("ROLLBACK")
        logging.error(f"Failed to delete data for user {user_id}: {e}")
        raise

# ===== DATA MINIMIZATION: Retenci√≥n autom√°tica =====
def apply_retention_policy():
    """
    Aplicar pol√≠ticas de retenci√≥n (eliminar datos viejos)
    """
    from snowflake.connector import connect

    conn = connect(...)
    cursor = conn.cursor()

    # Eliminar datos m√°s viejos que retention period
    cursor.execute("""
        DELETE FROM customers
        WHERE deleted_at IS NOT NULL
            AND deleted_at < DATEADD(day, -30, CURRENT_DATE)
    """)

    cursor.execute("""
        DELETE FROM orders
        WHERE order_date < DATEADD(year, -7, CURRENT_DATE)
    """)

    conn.commit()
```

### CCPA (California Consumer Privacy Act)

Similar a GDPR, con √©nfasis en:
- Right to know what data is collected
- Right to delete
- Right to opt-out of sale of data

### HIPAA (Health Insurance Portability and Accountability Act)

Para datos de salud (PHI - Protected Health Information):
- Encriptaci√≥n en tr√°nsito y en reposo
- Access logs y audit trails
- Backup y disaster recovery
- Business Associate Agreements (BAA)

---

## Herramientas y Frameworks

### Comparaci√≥n de Herramientas

| Herramienta | Tipo | Caracter√≠sticas | Costo |
|-------------|------|-----------------|-------|
| **Great Expectations** | Data validation | Open-source, Python, extensible | Free |
| **dbt** | Transformation + testing | SQL-based, CI/CD friendly | Free (core) |
| **Apache Atlas** | Metadata + lineage | Hadoop ecosystem | Free |
| **Collibra** | Enterprise governance | End-to-end platform | $$$ |
| **Alation** | Data catalog | ML-powered discovery | $$$ |
| **Monte Carlo** | Data observability | Anomaly detection | $$ |
| **Datadog** | Monitoring | Full-stack observability | $$ |

---

## üéØ Preguntas de Entrevista

**P: ¬øC√≥mo implementar√≠as data quality checks en un pipeline ETL?**

R:
1. **Pre-validaci√≥n**: Verificar schema y formato antes de procesar
2. **Great Expectations**: Definir expectation suites para cada dataset
3. **Airflow integration**: Task de validaci√≥n antes de load
4. **Post-validaci√≥n**: Checks en datos cargados (counts, aggregations)
5. **Alerting**: Slack/PagerDuty si validaci√≥n falla
6. **Quarantine**: Mover registros inv√°lidos a tabla separada para revisi√≥n

**P: ¬øQu√© har√≠as si un dashboard muestra m√©tricas incorrectas?**

R:
1. **Data lineage**: Rastrear de d√≥nde viene el dato
2. **Validation**: Verificar cada paso del pipeline (source ‚Üí transform ‚Üí dashboard)
3. **Compare with source**: Query directo a fuente vs dashboard
4. **Check transformations**: Revisar l√≥gica de agregaciones/joins
5. **Temporal analysis**: ¬øCu√°ndo empez√≥ a fallar? Cambios recientes?
6. **Root cause**: Identificar si es c√≥digo, datos, o configuraci√≥n

**P: Explica c√≥mo cumplir√≠as con GDPR en un data warehouse**

R:
1. **Data classification**: Identificar tablas con PII
2. **Encryption**: At-rest y in-transit para PII
3. **Access control**: RBAC, columnas PII solo accesibles con permiso
4. **Audit logging**: Log de accesos a PII
5. **Right to access**: API para exportar datos de usuario
6. **Right to erasure**: Proceso automatizado para eliminar/anonimizar
7. **Retention policies**: Auto-delete despu√©s de per√≠odo requerido
8. **Consent management**: Tracking de consentimiento de usuario

---

**Siguiente:** [10. Seguridad y Cumplimiento](10_seguridad_cumplimiento.md)
